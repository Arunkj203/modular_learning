
# model_config.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig ,StoppingCriteria, StoppingCriteriaList 
import os , requests , re , json

from transformers import GenerationConfig

# Optional: load .env
try:
    from dotenv import load_dotenv 
    load_dotenv()
except Exception:
    pass



# -----------------------------
# Config: model selection
# -----------------------------

BASE_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Llama-70B" # Larger Model


# BASE_MODEL = "HuggingFaceM4/tiny-random-LlamaForCausalLM"  # Test Model
OUTPUT_DIR = "./results/lora_adapters"



# DEVICE = "cuda"  # full GPU
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")


# Constants
Retries = 3


# 1. Define the Quantization Configuration for INT8
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0, # Optional: Adjust for better precision 
    llm_int8_enable_fp32_cpu_offload=True # Optional: Safety net
)


# -----------------------------
# Singleton loader
# -----------------------------


def get_model_and_tokenizer():

    print(f"Loading tokenizer for {BASE_MODEL}...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL,token=HUGGINGFACEHUB_API_TOKEN)

    print(f"Loading model {BASE_MODEL} on {DEVICE} (FP8)...")
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        quantization_config=bnb_config,
        dtype=torch.float16,
        token = HUGGINGFACEHUB_API_TOKEN
    )

    return model , tokenizer

# -----------------------------
# Helper function for generation
# -----------------------------


class StopOnToken(StoppingCriteria):
    def __init__(self, tokenizer, stop_token):
        self.tokenizer = tokenizer
        self.stop_token = stop_token
        self.stop_token_ids = tokenizer.encode(stop_token, add_special_tokens=False)
        
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        # Check if the last tokens match our stop token
        if input_ids.shape[1] < len(self.stop_token_ids):
            return False
            
        # Get the recent tokens that could match our stop token
        recent_tokens = input_ids[0, -len(self.stop_token_ids):].tolist()
        return recent_tokens == self.stop_token_ids

def generate_text(model, tokenizer, system_prompt, user_prompt, dynamic_max_tokens=200):
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    # We use skip_special_tokens=False here so the special tokens are included in the prompt
    prompt = tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
    )

    print("Prompt Created")

    last_error = None
    raw = None
    
    # Tokenize the final prompt
    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=True).to(model.device) # Use model.device for safety

    for attempt in range(Retries):

        try:
            
            print(f"Trying {attempt+1}")

            max_tokens = min(4096, dynamic_max_tokens * (2 ** attempt))
        
            gen_cfg = GenerationConfig(
                max_new_tokens=max_tokens,
                do_sample=False,
                # Setting pad_token_id to eos_token_id is a common Llama practice
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    generation_config=gen_cfg,
                    stopping_criteria=StoppingCriteriaList([
                        StopOnToken(tokenizer, "<end>")
                    ])
                )

            # 2. DECODE AND EXTRACT THE GENERATED TEXT
            # We only decode the *new* tokens generated by the model (after the prompt)
            generated_token_ids = outputs[0][inputs["input_ids"].shape[-1]:]
            raw = tokenizer.decode(generated_token_ids, skip_special_tokens=True)
            
            # Remove the <end> token if it was generated and used to stop the generation
            generated_text = raw.replace("<end>", "").strip()


            # 3. EXTRACT JSON ARRAY
            """
            Extract JSON array of primitives from raw LLM output wrapped with <start> and <end>
            """
            # Search for the <start> and <end> markers in the generated text
            # Use re.S (DOTALL) flag to match across newlines
            match = re.search(r"<start>(.*?)</end>", generated_text, flags=re.S)
            
            if not match:
                raise ValueError("Could not find <start>...</end> markers in the output.")

            json_text = match.group(1).strip()

            # Remove trailing commas before } or ] (a common fix for LLM-generated JSON)
            json_text = re.sub(r',\s*([\]\}])', r'\1', json_text)
            
            # Handle possible extra assistant tags that might be generated right after the JSON
            # For DeepSeek/Llama: The assistant tag is '<|Assistant|>'
            json_text = json_text.split(tokenizer.bos_token_id)[0].strip() # Clean up any garbage tokens
            
            # The next line uses json.loads() which is where a JSONDecodeError is most likely to occur.
            return json.loads(json_text)

        except Exception as e:
            last_error = e
            # Using print(repr(raw)) can be helpful to see non-printable characters
            print(f"[WARN] Attempt {attempt+1} failed: {e}\nRaw Output Snippet:\n{raw[:500].strip() if raw else 'None'}\n")
    else:
        # This executes if the loop completes without a successful 'return' (i.e., after all retries)
        raise RuntimeError(f"Failed after {Retries} attempts.\nError: {last_error}.\nRaw Output:\n{raw.strip() if raw else 'None'}")

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")


def call_openrouter(system_prompt: str, user_prompt: str, model="meta-llama/llama-3.3-70b-instruct:free", max_tokens=1000, temperature=0.0):
    """
    Call LLaMA-3.3-70B Instruct via OpenRouter API.
    """
    url = "https://openrouter.ai/api/v1/chat/completions"

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
    }

    response = requests.post(url, headers=headers, json=payload)
    response.raise_for_status()
    data = response.json()
    
    return data["choices"][0]["message"]["content"].strip()
